{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from proj2_helpers import *\n",
    "from get_embeddings_ML import *\n",
    "from ML_sklearn import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_EMB = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsize = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COOC_PATH = './Results/cooc.pkl'\n",
    "VOC_PATH = './Results/vocab.pkl'\n",
    "EMBEDDINGS_PATH = './Results/embeddings.npy'\n",
    "PP_POS_PATH = './Results/pp_pos_otpl_nd.txt'\n",
    "PP_NEG_PATH = './Results/pp_neg_otpl_nd.txt'\n",
    "PP_TEST_PATH = './Results/pp_test_otpl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pickle files\n",
    "cooc_matrix = open_pickle_file(COOC_PATH)\n",
    "vocabulary = open_pickle_file(VOC_PATH)\n",
    "\n",
    "# load numpy files \n",
    "embeddings = np.load(EMBEDDINGS_PATH)\n",
    "\n",
    "# load the data files = list with each line being a tweet\n",
    "pos = open(PP_POS_PATH, \"r\").read().splitlines()\n",
    "neg = open(PP_NEG_PATH, \"r\").read().splitlines()\n",
    "test = open(PP_TEST_PATH, \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> process pos and neg datas to get X and y to perform ML\n",
      "> extracting mean of word vectors\n",
      "> extracting mean of word vectors\n",
      "> X and y informations:\n",
      "X shape: (173211, 20)\n",
      "y shape: (173211,)\n"
     ]
    }
   ],
   "source": [
    "full_df, X, y = process_train_ML(pos, neg, vocabulary, embeddings, DIM_EMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Token_idx</th>\n",
       "      <th>Words_Vectors</th>\n",
       "      <th>Mean_Word_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20573</th>\n",
       "      <td>1</td>\n",
       "      <td>funny laugh joke</td>\n",
       "      <td>[222, 278, 531]</td>\n",
       "      <td>[[-0.12507859772682145, 0.1784135647220726, 0....</td>\n",
       "      <td>[-0.09047809048145776, 0.057583826583266544, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16337</th>\n",
       "      <td>1</td>\n",
       "      <td>please wish happy birthday love</td>\n",
       "      <td>[16, 34, 1, 91, 2]</td>\n",
       "      <td>[[0.030061481699418236, 0.5114536498788929, 0....</td>\n",
       "      <td>[0.02329364021277915, 0.30944829278068897, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162885</th>\n",
       "      <td>-1</td>\n",
       "      <td>test test japan ask gaijin card ask gain</td>\n",
       "      <td>[326, 326, 1463, 129, 422, 129, 1409]</td>\n",
       "      <td>[[0.35190521898842136, 0.9881142814841043, 0.1...</td>\n",
       "      <td>[-0.027867567107210674, 0.1074513059316159, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84251</th>\n",
       "      <td>1</td>\n",
       "      <td>hahahahah happy without entire necessity</td>\n",
       "      <td>[3023, 1, 207, 1917, 9389]</td>\n",
       "      <td>[[0.020253429545624476, 0.03013228395656892, -...</td>\n",
       "      <td>[-0.4859748043027599, -0.15808217277733674, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65905</th>\n",
       "      <td>1</td>\n",
       "      <td>head eye close everything sadly</td>\n",
       "      <td>[175, 249, 341, 149, 1244]</td>\n",
       "      <td>[[-0.2953530624881164, -0.09312038956597625, 0...</td>\n",
       "      <td>[-0.4182472446210581, 0.15333816786590043, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment                                     Tweet  \\\n",
       "20573           1                          funny laugh joke   \n",
       "16337           1           please wish happy birthday love   \n",
       "162885         -1  test test japan ask gaijin card ask gain   \n",
       "84251           1  hahahahah happy without entire necessity   \n",
       "65905           1           head eye close everything sadly   \n",
       "\n",
       "                                    Token_idx  \\\n",
       "20573                         [222, 278, 531]   \n",
       "16337                      [16, 34, 1, 91, 2]   \n",
       "162885  [326, 326, 1463, 129, 422, 129, 1409]   \n",
       "84251              [3023, 1, 207, 1917, 9389]   \n",
       "65905              [175, 249, 341, 149, 1244]   \n",
       "\n",
       "                                            Words_Vectors  \\\n",
       "20573   [[-0.12507859772682145, 0.1784135647220726, 0....   \n",
       "16337   [[0.030061481699418236, 0.5114536498788929, 0....   \n",
       "162885  [[0.35190521898842136, 0.9881142814841043, 0.1...   \n",
       "84251   [[0.020253429545624476, 0.03013228395656892, -...   \n",
       "65905   [[-0.2953530624881164, -0.09312038956597625, 0...   \n",
       "\n",
       "                                         Mean_Word_Vector  \n",
       "20573   [-0.09047809048145776, 0.057583826583266544, 0...  \n",
       "16337   [0.02329364021277915, 0.30944829278068897, 0.1...  \n",
       "162885  [-0.027867567107210674, 0.1074513059316159, 0....  \n",
       "84251   [-0.4859748043027599, -0.15808217277733674, -0...  \n",
       "65905   [-0.4182472446210581, 0.15333816786590043, 0.1...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check full_df\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> splitting datas into train and test sets\n",
      "> standardizing\n",
      "> performing PCA\n",
      "Train set: X shape:  (138568, 19) y shape: (138568,)\n",
      "Test set: X shape:  (34643, 19) y shape: (34643,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, scaler, pca = split_standardize_pca(X, y, testsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5894408682850792\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='adam', alpha=1e-4, hidden_layer_sizes=(2, X_train.shape[1]), random_state=4, verbose=False, learning_rate='constant')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "compute_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> process test data to get X_test and perform ML\n",
      "> extracting mean of word vectors\n",
      "> X_test informations:\n",
      "X_test shape: (10000, 20)\n"
     ]
    }
   ],
   "source": [
    "df_test, testx = process_test_ML(test, vocabulary, embeddings, DIM_EMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testx = std_pca_test(testx, scaler, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = clf.predict(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify length of test:  10000\n"
     ]
    }
   ],
   "source": [
    "create_submission(df_test, y_pred_test, './Submissions/NN_we.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
