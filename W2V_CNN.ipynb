{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.layers import Conv1D, Flatten\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import text\n",
    "from proj2_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_POS_PATH = './Results/pp_pos_otpl_nd.txt'\n",
    "RESULT_NEG_PATH = './Results/pp_neg_otpl_nd.txt'\n",
    "RES_PATH = './Results/pp_test_otpl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data files = list with each line being a tweet\n",
    "result_pos = open(RESULT_POS_PATH, \"r\").read().splitlines()\n",
    "result_neg = open(RESULT_NEG_PATH, \"r\").read().splitlines()\n",
    "test_set = open(RES_PATH, \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAFRAME CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>CNN_Labels</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>68517</td>\n",
       "      <td>1</td>\n",
       "      <td>mao keep taste silly</td>\n",
       "      <td>1</td>\n",
       "      <td>[mao, keep, taste, silly]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58313</td>\n",
       "      <td>1</td>\n",
       "      <td>happy birthday fool</td>\n",
       "      <td>1</td>\n",
       "      <td>[happy, birthday, fool]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5585</td>\n",
       "      <td>1</td>\n",
       "      <td>morning thanking man another day another chanc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[morning, thanking, man, another, day, another...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51280</td>\n",
       "      <td>1</td>\n",
       "      <td>hour live australia right waiting</td>\n",
       "      <td>1</td>\n",
       "      <td>[hour, live, australia, right, waiting]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135379</td>\n",
       "      <td>-1</td>\n",
       "      <td>ago rate sanity</td>\n",
       "      <td>0</td>\n",
       "      <td>[ago, rate, sanity]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16006</td>\n",
       "      <td>1</td>\n",
       "      <td>yay wanna ask something lee dmdmdmdm</td>\n",
       "      <td>1</td>\n",
       "      <td>[yay, wan, na, ask, something, lee, dmdmdmdm]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17693</td>\n",
       "      <td>1</td>\n",
       "      <td>favourite tweet</td>\n",
       "      <td>1</td>\n",
       "      <td>[favourite, tweet]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10342</td>\n",
       "      <td>1</td>\n",
       "      <td>ask massage therapy physical therapy help</td>\n",
       "      <td>1</td>\n",
       "      <td>[ask, massage, therapy, physical, therapy, help]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135183</td>\n",
       "      <td>-1</td>\n",
       "      <td>pathogen wild farmed fish sea louse elli horwo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[pathogen, wild, farmed, fish, sea, louse, ell...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142965</td>\n",
       "      <td>-1</td>\n",
       "      <td>knackered netball hard work sit watch even bar...</td>\n",
       "      <td>0</td>\n",
       "      <td>[knackered, netball, hard, work, sit, watch, e...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173211 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment                                              Tweet  \\\n",
       "68517           1                               mao keep taste silly   \n",
       "58313           1                                happy birthday fool   \n",
       "5585            1  morning thanking man another day another chanc...   \n",
       "51280           1                  hour live australia right waiting   \n",
       "135379         -1                                    ago rate sanity   \n",
       "...           ...                                                ...   \n",
       "16006           1               yay wanna ask something lee dmdmdmdm   \n",
       "17693           1                                    favourite tweet   \n",
       "10342           1          ask massage therapy physical therapy help   \n",
       "135183         -1  pathogen wild farmed fish sea louse elli horwo...   \n",
       "142965         -1  knackered netball hard work sit watch even bar...   \n",
       "\n",
       "        CNN_Labels                                             tokens  Pos  \\\n",
       "68517            1                          [mao, keep, taste, silly]    1   \n",
       "58313            1                            [happy, birthday, fool]    1   \n",
       "5585             1  [morning, thanking, man, another, day, another...    1   \n",
       "51280            1            [hour, live, australia, right, waiting]    1   \n",
       "135379           0                                [ago, rate, sanity]    0   \n",
       "...            ...                                                ...  ...   \n",
       "16006            1      [yay, wan, na, ask, something, lee, dmdmdmdm]    1   \n",
       "17693            1                                 [favourite, tweet]    1   \n",
       "10342            1   [ask, massage, therapy, physical, therapy, help]    1   \n",
       "135183           0  [pathogen, wild, farmed, fish, sea, louse, ell...    0   \n",
       "142965           0  [knackered, netball, hard, work, sit, watch, e...    0   \n",
       "\n",
       "        Neg  \n",
       "68517     0  \n",
       "58313     0  \n",
       "5585      0  \n",
       "51280     0  \n",
       "135379    1  \n",
       "...     ...  \n",
       "16006     0  \n",
       "17693     0  \n",
       "10342     0  \n",
       "135183    1  \n",
       "142965    1  \n",
       "\n",
       "[173211 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------------------------------TRAINING SET---------------------------------------------------------------------------\n",
    "\n",
    "# create labels\n",
    "label_pos = [1] * len(result_pos)\n",
    "#create a df\n",
    "pos_df = pd.DataFrame(list(zip(label_pos, result_pos)),columns=[\"Sentiment\",\"Tweet\"]) \n",
    "del label_pos\n",
    "\n",
    "# create labels\n",
    "label_neg = [-1] * len(result_neg)\n",
    "# create a df\n",
    "neg_df = pd.DataFrame(list(zip(label_neg, result_neg)),columns=[\"Sentiment\",\"Tweet\"]) #create a df\n",
    "del label_neg\n",
    "\n",
    "# regroup the dfs, ignore index in order to get new ones (->no duplicate)\n",
    "train_df = pd.concat([pos_df,neg_df],ignore_index=True) #regroup the dfs, ignore index in order to get new ones (->no duplicate)\n",
    "\n",
    "train_tokens = [word_tokenize(sen) for sen in train_df.Tweet] \n",
    "\n",
    "train_df['tokens'] = train_tokens\n",
    "\n",
    "CNNLabel = [0 if val == -1 else 1 for val in train_df.Sentiment.values]\n",
    "\n",
    "train_df.insert(2,\"CNN_Labels\",CNNLabel)\n",
    "\n",
    "# shuffle the rows\n",
    "train_df = train_df.sample(frac=1) \n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "for l in train_df.CNN_Labels:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        \n",
    "train_df['Pos']= pos\n",
    "train_df['Neg']= neg\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_submission_id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>sea doo pro sea scooter sport portable sea doo...</td>\n",
       "      <td>[sea, doo, pro, sea, scooter, sport, portable,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>shuck well work week come cheer put battery ca...</td>\n",
       "      <td>[shuck, well, work, week, come, cheer, put, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>stay away bug that baby</td>\n",
       "      <td>[stay, away, bug, that, baby]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>madam lol perfectly fine contagious anymore mao</td>\n",
       "      <td>[madam, lol, perfectly, fine, contagious, anym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>whenever fall asleep watch always wake headache</td>\n",
       "      <td>[whenever, fall, asleep, watch, always, wake, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>9996</td>\n",
       "      <td>nice time friend lastnite</td>\n",
       "      <td>[nice, time, friend, lastnite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>9997</td>\n",
       "      <td>please stop</td>\n",
       "      <td>[please, stop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>9998</td>\n",
       "      <td>without daughter two time oscar winner sally f...</td>\n",
       "      <td>[without, daughter, two, time, oscar, winner, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>9999</td>\n",
       "      <td>fun class sweetcheeks</td>\n",
       "      <td>[fun, class, sweetcheeks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>10000</td>\n",
       "      <td>make difference get recreational leadership pr...</td>\n",
       "      <td>[make, difference, get, recreational, leadersh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tweet_submission_id                                              Tweet  \\\n",
       "0                       1  sea doo pro sea scooter sport portable sea doo...   \n",
       "1                       2  shuck well work week come cheer put battery ca...   \n",
       "2                       3                            stay away bug that baby   \n",
       "3                       4    madam lol perfectly fine contagious anymore mao   \n",
       "4                       5    whenever fall asleep watch always wake headache   \n",
       "...                   ...                                                ...   \n",
       "9995                 9996                          nice time friend lastnite   \n",
       "9996                 9997                                        please stop   \n",
       "9997                 9998  without daughter two time oscar winner sally f...   \n",
       "9998                 9999                              fun class sweetcheeks   \n",
       "9999                10000  make difference get recreational leadership pr...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [sea, doo, pro, sea, scooter, sport, portable,...  \n",
       "1     [shuck, well, work, week, come, cheer, put, ba...  \n",
       "2                         [stay, away, bug, that, baby]  \n",
       "3     [madam, lol, perfectly, fine, contagious, anym...  \n",
       "4     [whenever, fall, asleep, watch, always, wake, ...  \n",
       "...                                                 ...  \n",
       "9995                     [nice, time, friend, lastnite]  \n",
       "9996                                     [please, stop]  \n",
       "9997  [without, daughter, two, time, oscar, winner, ...  \n",
       "9998                          [fun, class, sweetcheeks]  \n",
       "9999  [make, difference, get, recreational, leadersh...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------------------------------TEST SET---------------------------------------------------------------------------\n",
    "test_ids = np.linspace(1,10000,10000, dtype=int)\n",
    "# create a df\n",
    "test_df = pd.DataFrame(list(zip(test_ids, test_set)), columns=[\"Tweet_submission_id\",\"Tweet\"]) \n",
    "\n",
    "test_tokens = [word_tokenize(sen) for sen in test_df.Tweet] \n",
    "\n",
    "test_df['tokens'] = test_tokens\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(train_df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960179 words total, with a vocabulary size of 38011\n",
      "Max sentence length is 24\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239398 words total, with a vocabulary size of 19718\n",
      "Max sentence length is 26\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38010 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Tweet\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Tweet\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38011, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Tweet\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_cnn_data\n",
    "X_test = test_cnn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Pos','Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values\n",
    "y_test = data_test[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 300)      11403300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 49, 200)      120200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 48, 200)      180200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 47, 200)      240200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 46, 200)      300200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 45, 200)      360200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000)         0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          128128      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,732,686\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 11,403,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110854 samples, validate on 27714 samples\n",
      "Epoch 1/3\n",
      "110854/110854 [==============================] - 872s 8ms/step - loss: 0.5024 - acc: 0.7453 - val_loss: 0.4811 - val_acc: 0.7666\n",
      "Epoch 2/3\n",
      "110854/110854 [==============================] - 856s 8ms/step - loss: 0.4444 - acc: 0.7847 - val_loss: 0.4678 - val_acc: 0.7692\n",
      "Epoch 3/3\n",
      "110854/110854 [==============================] - 809s 7ms/step - loss: 0.3946 - acc: 0.8132 - val_loss: 0.4804 - val_acc: 0.7681\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, epochs=num_epochs, validation_split=0.2, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34643/34643 [==============================] - 83s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.7029389e-01, 2.2979766e-01],\n",
       "       [5.1655674e-01, 4.8287201e-01],\n",
       "       [9.0799481e-01, 9.1882288e-02],\n",
       "       ...,\n",
       "       [4.6922418e-01, 5.3029549e-01],\n",
       "       [3.0102835e-03, 9.9710280e-01],\n",
       "       [7.5116745e-06, 9.9999285e-01]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.766215397049909"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_test.CNN_Labels==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17538\n",
       "1    17105\n",
       "Name: CNN_Labels, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.CNN_Labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68947 words total, with a vocabulary size of 9748\n",
      "Max sentence length is 19\n"
     ]
    }
   ],
   "source": [
    "all_Test_words = [word for tokens in test_df[\"tokens\"] for word in tokens]\n",
    "Test_sentence_lengths = [len(tokens) for tokens in test_df[\"tokens\"]]\n",
    "r_TEST_VOCAB = sorted(list(set(all_Test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_Test_words), len(r_TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(Test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_sequences = tokenizer.texts_to_sequences(test_df[\"Tweet\"].tolist())\n",
    "Test_lr = pad_sequences(Test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 25s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "r_y_pred = model.predict(Test_lr, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]\n",
    "r_prediction_labels=[]\n",
    "for p in r_y_pred:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_prediction_labels = [-1 if pred == 0 else 1 for pred in prediction_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df['Tweet_submission_id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_id,r_prediction_labels, \"./Submissions/W2V_CNN.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
