{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from get_GloVe_emb_ML import *\n",
    "from proj2_helpers import *\n",
    "from ML_CNN import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP_POS_PATH = './Results/pp_pos_otpl_nd.txt' \n",
    "PP_NEG_PATH = './Results/pp_neg_otpl_nd.txt' \n",
    "PP_TEST_PATH = './Results/pp_test_otpl.txt'\n",
    "VECTORS_PATH = './GloVe-1.2/vectors.txt'\n",
    "VOCABULARY_PATH = './GloVe-1.2/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {} \n",
    "with open(VOCABULARY_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        tokens = line.strip().split()\n",
    "        vocabulary[tokens[0]] = [float(x) for x in tokens[1:]]\n",
    "vocabulary = {k: v[0] for k, v in vocabulary.items()}\n",
    "VOCAB_SIZE = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_EMB = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsize = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = open(PP_POS_PATH, \"r\").read().splitlines()\n",
    "neg = open(PP_NEG_PATH, \"r\").read().splitlines()\n",
    "test = open(PP_TEST_PATH, \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = get_vectors(VECTORS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> process pos and neg datas to get X and y to perform ML\n",
      "> computing mean of word vectors\n",
      "> computing mean of word vectors\n",
      "> X and y informations:\n",
      "X shape: (173209, 50)\n",
      "y shape: (173209,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>twt_vec</th>\n",
       "      <th>twt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>44546</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.327424375, 0.20842725, -0.19167375, -0.054...</td>\n",
       "      <td>smile outside one thing smile inside another h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20501</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.17488874999999998, 0.238007, 0.15211524999...</td>\n",
       "      <td>absolutely sure trust gene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60652</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.12295224999999999, 0.27231737500000003, -0....</td>\n",
       "      <td>offline gon instagram come follow help usernam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3207</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.18450114285714284, 0.39531614285714284, -0...</td>\n",
       "      <td>like sweet excite face straight board bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34112</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.3030275714285714, 0.09224271428571426, -0....</td>\n",
       "      <td>plenty fish sea stay fuck away nemo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            twt_vec  \\\n",
       "44546          1  [-0.327424375, 0.20842725, -0.19167375, -0.054...   \n",
       "20501          1  [-0.17488874999999998, 0.238007, 0.15211524999...   \n",
       "60652          1  [0.12295224999999999, 0.27231737500000003, -0....   \n",
       "3207           1  [-0.18450114285714284, 0.39531614285714284, -0...   \n",
       "34112          1  [-0.3030275714285714, 0.09224271428571426, -0....   \n",
       "\n",
       "                                                     twt  \n",
       "44546  smile outside one thing smile inside another h...  \n",
       "20501                         absolutely sure trust gene  \n",
       "60652  offline gon instagram come follow help usernam...  \n",
       "3207           like sweet excite face straight board bus  \n",
       "34112                plenty fish sea stay fuck away nemo  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df, X, y = get_train_emb_CNN(pos, neg, vectors, DIM_EMB)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split + PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = full_df.copy()\n",
    "X_df = X_df.drop(['sentiment'],axis=1)\n",
    "y_df = full_df.copy()\n",
    "y_df = y_df.drop(['twt_vec','twt'],axis=1)\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size=testsize, random_state=1)\n",
    "X_train = np.vstack(np.array(X_train_df['twt_vec'].values))\n",
    "X_test = np.vstack(np.array(X_test_df['twt_vec'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray([[0,1] if y == -1 else [1,0] for y in y_train_df['sentiment'].values])\n",
    "y_test = np.asarray([[0,1] if y == -1 else [1,0] for y in y_test_df['sentiment'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> performing PCA\n",
      "Train set: X shape:  (138567, 40) y shape: (138567, 2)\n",
      "Test set: X shape:  (34642, 40) y shape: (34642, 2)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "# fit on train set only\n",
    "scaler.fit(X_train)\n",
    "# apply transform to train and test\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print('> performing PCA')\n",
    "# create instance of PCA\n",
    "pca = PCA(.95)\n",
    "# fit PCA on train set only\n",
    "pca.fit(X_train)\n",
    "# apply on train and test \n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print('Train set: X shape: ', X_train.shape, 'y shape:', y_train.shape)\n",
    "print('Test set: X shape: ', X_test.shape, 'y shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138568, 50)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(X_train)+1, EMB_DIM))\n",
    "for index, word in enumerate(X_train_df['twt']):\n",
    "    train_embedding_weights[index,:] = vectors[word] if word in vectors else np.random.rand(EMB_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38017 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(vocabulary), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(X_train_df['twt'].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train_df['twt'].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(X_test_df['twt'].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 50, 50)       6928400     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 49, 200)      20200       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 48, 200)      30200       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 47, 200)      40200       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 46, 200)      50200       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 45, 200)      60200       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 200)          0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 200)          0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 200)          0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 200)          0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 200)          0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1000)         0           global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          128128      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            258         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,257,786\n",
      "Trainable params: 329,386\n",
      "Non-trainable params: 6,928,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(X_train)+1, EMB_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124710 samples, validate on 13857 samples\n",
      "Epoch 1/3\n",
      "124710/124710 [==============================] - 237s 2ms/step - loss: 0.6202 - acc: 0.6434 - val_loss: 0.5614 - val_acc: 0.7009\n",
      "Epoch 2/3\n",
      "124710/124710 [==============================] - 244s 2ms/step - loss: 0.5604 - acc: 0.7031 - val_loss: 0.5446 - val_acc: 0.7132\n",
      "Epoch 3/3\n",
      "124710/124710 [==============================] - 247s 2ms/step - loss: 0.5416 - acc: 0.7167 - val_loss: 0.5532 - val_acc: 0.7061\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_cnn_data, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34642/34642 [==============================] - 17s 498us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df[\"cnn_label\"] = [0 if sent == -1 else 1 for sent in y_test_df[\"sentiment\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7072339934183939"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test_df.cnn_label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> process test data to get X_test and perform ML\n",
      "> computing mean of word vectors\n",
      "Test shape (10000, 50)\n"
     ]
    }
   ],
   "source": [
    "test_df, test_x = get_test_emb(test, vectors, DIM_EMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_submission_id</th>\n",
       "      <th>twt_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.16490428571428575, 0.07959978571428572, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.21977688888888888, 0.31933866666666666, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.14929240000000002, 0.5985406, -0.461216, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.29876600000000003, 0.3397075714285714, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[-0.20306328571428572, 0.43171842857142856, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>9996</td>\n",
       "      <td>[-0.22906875000000002, 0.37710675000000005, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>9997</td>\n",
       "      <td>[0.21504649999999997, 0.487047, -0.2139565, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>9998</td>\n",
       "      <td>[-0.0004636153846153871, 0.19289269230769232, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>9999</td>\n",
       "      <td>[-0.39865766666666674, 0.216822, 0.03363399999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>10000</td>\n",
       "      <td>[-0.3815797777777778, 0.014435222222222254, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tweet_submission_id                                            twt_vec\n",
       "0                       1  [-0.16490428571428575, 0.07959978571428572, -0...\n",
       "1                       2  [-0.21977688888888888, 0.31933866666666666, -0...\n",
       "2                       3  [-0.14929240000000002, 0.5985406, -0.461216, 0...\n",
       "3                       4  [-0.29876600000000003, 0.3397075714285714, -0....\n",
       "4                       5  [-0.20306328571428572, 0.43171842857142856, -0...\n",
       "...                   ...                                                ...\n",
       "9995                 9996  [-0.22906875000000002, 0.37710675000000005, -0...\n",
       "9996                 9997  [0.21504649999999997, 0.487047, -0.2139565, 0....\n",
       "9997                 9998  [-0.0004636153846153871, 0.19289269230769232, ...\n",
       "9998                 9999  [-0.39865766666666674, 0.216822, 0.03363399999...\n",
       "9999                10000  [-0.3815797777777778, 0.014435222222222254, 0....\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_df.drop(columns=[\"Words_Vectors\",\"Mean_Word_Vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_PATH = './Results/pp_test_otpl.txt'\n",
    "test_set = open(RES_PATH, \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_set)\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 285us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels = [-1 if pred == 0 else 1 for pred in prediction_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df['Tweet_submission_id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_id,prediction_labels, \"./Submissions/CNN_GloVe.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
