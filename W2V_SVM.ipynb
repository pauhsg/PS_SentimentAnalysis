{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from proj2_helpers import *\n",
    "from get_embeddings_ML import *\n",
    "from ML_sklearn import *\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_POS_PATH = './Results/pp_pos_otpl_nd.txt'\n",
    "RESULT_NEG_PATH = './Results/pp_neg_otpl_nd.txt'\n",
    "RES_PATH = './Results/pp_test_otpl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data files = list with each line being a tweet\n",
    "result_pos = open(RESULT_POS_PATH, \"r\").read().splitlines()\n",
    "result_neg = open(RESULT_NEG_PATH, \"r\").read().splitlines()\n",
    "test_set = open(RES_PATH, \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAFRAME CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------TRAINING SET---------------------------------------------------------------------------\n",
    "\n",
    "# create labels\n",
    "label_pos = [1] * len(result_pos)\n",
    "#create a df\n",
    "pos_df = pd.DataFrame(list(zip(label_pos, result_pos)),columns=[\"Sentiment\",\"Tweet\"]) \n",
    "del label_pos\n",
    "\n",
    "# create labels\n",
    "label_neg = [-1] * len(result_neg)\n",
    "# create a df\n",
    "neg_df = pd.DataFrame(list(zip(label_neg, result_neg)),columns=[\"Sentiment\",\"Tweet\"]) #create a df\n",
    "del label_neg\n",
    "\n",
    "# regroup the dfs, ignore index in order to get new ones (->no duplicate)\n",
    "train_df = pd.concat([pos_df,neg_df],ignore_index=True) #regroup the dfs, ignore index in order to get new ones (->no duplicate)\n",
    "\n",
    "train_tokens = [word_tokenize(sen) for sen in train_df.Tweet] \n",
    "\n",
    "train_df['tokens'] = train_tokens\n",
    "\n",
    "# shuffle the rows\n",
    "train_df = train_df.sample(frac=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>22088</td>\n",
       "      <td>1</td>\n",
       "      <td>follow please mind</td>\n",
       "      <td>[follow, please, mind]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92355</td>\n",
       "      <td>-1</td>\n",
       "      <td>schiller collection american art song song com...</td>\n",
       "      <td>[schiller, collection, american, art, song, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149910</td>\n",
       "      <td>-1</td>\n",
       "      <td>home rather bored shall message</td>\n",
       "      <td>[home, rather, bored, shall, message]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115603</td>\n",
       "      <td>-1</td>\n",
       "      <td>picture frame poster frame wide complete matte...</td>\n",
       "      <td>[picture, frame, poster, frame, wide, complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148829</td>\n",
       "      <td>-1</td>\n",
       "      <td>pip guru trade say happy</td>\n",
       "      <td>[pip, guru, trade, say, happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37842</td>\n",
       "      <td>1</td>\n",
       "      <td>one less person worry lol</td>\n",
       "      <td>[one, less, person, worry, lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59688</td>\n",
       "      <td>1</td>\n",
       "      <td>sound like auntie kris good mommy</td>\n",
       "      <td>[sound, like, auntie, kris, good, mommy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154983</td>\n",
       "      <td>-1</td>\n",
       "      <td>would love snuggle boo hoo downer time sleep</td>\n",
       "      <td>[would, love, snuggle, boo, hoo, downer, time,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106134</td>\n",
       "      <td>-1</td>\n",
       "      <td>reese skinless boneless sardine olive oil ounc...</td>\n",
       "      <td>[reese, skinless, boneless, sardine, olive, oi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8179</td>\n",
       "      <td>1</td>\n",
       "      <td>always beautiful</td>\n",
       "      <td>[always, beautiful]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173211 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment                                              Tweet  \\\n",
       "22088           1                                 follow please mind   \n",
       "92355          -1  schiller collection american art song song com...   \n",
       "149910         -1                    home rather bored shall message   \n",
       "115603         -1  picture frame poster frame wide complete matte...   \n",
       "148829         -1                           pip guru trade say happy   \n",
       "...           ...                                                ...   \n",
       "37842           1                          one less person worry lol   \n",
       "59688           1                  sound like auntie kris good mommy   \n",
       "154983         -1       would love snuggle boo hoo downer time sleep   \n",
       "106134         -1  reese skinless boneless sardine olive oil ounc...   \n",
       "8179            1                                   always beautiful   \n",
       "\n",
       "                                                   tokens  \n",
       "22088                              [follow, please, mind]  \n",
       "92355   [schiller, collection, american, art, song, so...  \n",
       "149910              [home, rather, bored, shall, message]  \n",
       "115603  [picture, frame, poster, frame, wide, complete...  \n",
       "148829                     [pip, guru, trade, say, happy]  \n",
       "...                                                   ...  \n",
       "37842                     [one, less, person, worry, lol]  \n",
       "59688            [sound, like, auntie, kris, good, mommy]  \n",
       "154983  [would, love, snuggle, boo, hoo, downer, time,...  \n",
       "106134  [reese, skinless, boneless, sardine, olive, oi...  \n",
       "8179                                  [always, beautiful]  \n",
       "\n",
       "[173211 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------TEST SET---------------------------------------------------------------------------\n",
    "test_ids = np.linspace(1,10000,10000, dtype=int)\n",
    "# create a df\n",
    "test_df = pd.DataFrame(list(zip(test_ids, test_set)), columns=[\"Tweet_submission_id\",\"Tweet\"]) \n",
    "\n",
    "test_tokens = [word_tokenize(sen) for sen in test_df.Tweet] \n",
    "\n",
    "test_df['tokens'] = test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(train_df, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1079881 words total, with a vocabulary size of 40088\n",
      "Max sentence length is 26\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119696 words total, with a vocabulary size of 13910\n",
      "Max sentence length is 20\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40087 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Tweet\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Tweet\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40088, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_svm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train.Sentiment.values\n",
    "y_test = data_test.Sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Tweet\"].tolist())\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# fit on train set only\n",
    "scaler.fit(X_train)\n",
    "# apply transform to train and test\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# create instance of PCA\n",
    "pca = PCA(.95)\n",
    "# fit PCA on train set only\n",
    "pca.fit(X_train)\n",
    "# apply on train and test \n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5143747835122965\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "compute_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68947 words total, with a vocabulary size of 9748\n",
      "Max sentence length is 19\n"
     ]
    }
   ],
   "source": [
    "all_Test_words = [word for tokens in test_df[\"tokens\"] for word in tokens]\n",
    "Test_sentence_lengths = [len(tokens) for tokens in test_df[\"tokens\"]]\n",
    "r_TEST_VOCAB = sorted(list(set(all_Test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_Test_words), len(r_TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(Test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_sequences = tokenizer.texts_to_sequences(test_df[\"Tweet\"].tolist())\n",
    "Test_svm = pad_sequences(Test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_y_pred = svc.predict(Test_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df['Tweet_submission_id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_id,r_y_pred, \"./Submissions/SVM_W2V_SUB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
