{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from proj2_helpers import *\n",
    "from get_embeddings_ML import *\n",
    "from ML_sklearn import *\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_POS_PATH = './Results/pp_pos_otpl_nd.txt'\n",
    "RESULT_NEG_PATH = './Results/pp_neg_otpl_nd.txt'\n",
    "RES_PATH = './Results/pp_test_otpl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data files = list with each line being a tweet\n",
    "result_pos = open(RESULT_POS_PATH, \"r\").read().splitlines()\n",
    "result_neg = open(RESULT_NEG_PATH, \"r\").read().splitlines()\n",
    "test_set = open(RES_PATH, \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAFRAME CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------TRAINING SET---------------------------------------------------------------------------\n",
    "\n",
    "# create labels\n",
    "label_pos = [1] * len(result_pos)\n",
    "#create a df\n",
    "pos_df = pd.DataFrame(list(zip(label_pos, result_pos)),columns=[\"Sentiment\",\"Tweet\"]) \n",
    "del label_pos\n",
    "\n",
    "# create labels\n",
    "label_neg = [-1] * len(result_neg)\n",
    "# create a df\n",
    "neg_df = pd.DataFrame(list(zip(label_neg, result_neg)),columns=[\"Sentiment\",\"Tweet\"]) #create a df\n",
    "del label_neg\n",
    "\n",
    "# regroup the dfs, ignore index in order to get new ones (->no duplicate)\n",
    "train_df = pd.concat([pos_df,neg_df],ignore_index=True) #regroup the dfs, ignore index in order to get new ones (->no duplicate)\n",
    "\n",
    "train_tokens = [word_tokenize(sen) for sen in train_df.Tweet] \n",
    "\n",
    "train_df['tokens'] = train_tokens\n",
    "\n",
    "# shuffle the rows\n",
    "train_df = train_df.sample(frac=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>159233</td>\n",
       "      <td>-1</td>\n",
       "      <td>season finale waterloo road life</td>\n",
       "      <td>[season, finale, waterloo, road, life]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63242</td>\n",
       "      <td>1</td>\n",
       "      <td>god hear dear regard fam ffb pin pls lose</td>\n",
       "      <td>[god, hear, dear, regard, fam, ffb, pin, pls, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168215</td>\n",
       "      <td>-1</td>\n",
       "      <td>bad ground could get food</td>\n",
       "      <td>[bad, ground, could, get, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165946</td>\n",
       "      <td>-1</td>\n",
       "      <td>new look sewing pattern miss dress size miss d...</td>\n",
       "      <td>[new, look, sewing, pattern, miss, dress, size...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108707</td>\n",
       "      <td>-1</td>\n",
       "      <td>sorry danny lancaster tonight need</td>\n",
       "      <td>[sorry, danny, lancaster, tonight, need]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109339</td>\n",
       "      <td>-1</td>\n",
       "      <td>mitac mio minisync mobile charge kit high curr...</td>\n",
       "      <td>[mitac, mio, minisync, mobile, charge, kit, hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38657</td>\n",
       "      <td>1</td>\n",
       "      <td>depend quality good need one get one</td>\n",
       "      <td>[depend, quality, good, need, one, get, one]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164762</td>\n",
       "      <td>-1</td>\n",
       "      <td>city star war run laptop</td>\n",
       "      <td>[city, star, war, run, laptop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145596</td>\n",
       "      <td>-1</td>\n",
       "      <td>gon without shah every sunday</td>\n",
       "      <td>[gon, without, shah, every, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116116</td>\n",
       "      <td>-1</td>\n",
       "      <td>internet phone int work work wife guy</td>\n",
       "      <td>[internet, phone, int, work, work, wife, guy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173211 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment                                              Tweet  \\\n",
       "159233         -1                   season finale waterloo road life   \n",
       "63242           1          god hear dear regard fam ffb pin pls lose   \n",
       "168215         -1                          bad ground could get food   \n",
       "165946         -1  new look sewing pattern miss dress size miss d...   \n",
       "108707         -1                 sorry danny lancaster tonight need   \n",
       "...           ...                                                ...   \n",
       "109339         -1  mitac mio minisync mobile charge kit high curr...   \n",
       "38657           1               depend quality good need one get one   \n",
       "164762         -1                           city star war run laptop   \n",
       "145596         -1                      gon without shah every sunday   \n",
       "116116         -1              internet phone int work work wife guy   \n",
       "\n",
       "                                                   tokens  \n",
       "159233             [season, finale, waterloo, road, life]  \n",
       "63242   [god, hear, dear, regard, fam, ffb, pin, pls, ...  \n",
       "168215                    [bad, ground, could, get, food]  \n",
       "165946  [new, look, sewing, pattern, miss, dress, size...  \n",
       "108707           [sorry, danny, lancaster, tonight, need]  \n",
       "...                                                   ...  \n",
       "109339  [mitac, mio, minisync, mobile, charge, kit, hi...  \n",
       "38657        [depend, quality, good, need, one, get, one]  \n",
       "164762                     [city, star, war, run, laptop]  \n",
       "145596                [gon, without, shah, every, sunday]  \n",
       "116116      [internet, phone, int, work, work, wife, guy]  \n",
       "\n",
       "[173211 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------TEST SET---------------------------------------------------------------------------\n",
    "test_ids = np.linspace(1,10000,10000, dtype=int)\n",
    "# create a df\n",
    "test_df = pd.DataFrame(list(zip(test_ids, test_set)), columns=[\"Tweet_submission_id\",\"Tweet\"]) \n",
    "\n",
    "test_tokens = [word_tokenize(sen) for sen in test_df.Tweet] \n",
    "\n",
    "test_df['tokens'] = test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(train_df, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080474 words total, with a vocabulary size of 40166\n",
      "Max sentence length is 26\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119103 words total, with a vocabulary size of 13829\n",
      "Max sentence length is 20\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40165 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Tweet\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Tweet\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40166, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_nn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train.Sentiment.values\n",
    "y_test = data_test.Sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Tweet\"].tolist())\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# fit on train set only\n",
    "scaler.fit(X_train)\n",
    "# apply transform to train and test\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# create instance of PCA\n",
    "pca = PCA(.95)\n",
    "# fit PCA on train set only\n",
    "pca.fit(X_train)\n",
    "# apply on train and test \n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5751068006003925\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='adam', alpha=1e-4, hidden_layer_sizes=(2, X_train.shape[1]), random_state=4, verbose=False, learning_rate='constant')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "compute_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68947 words total, with a vocabulary size of 9748\n",
      "Max sentence length is 19\n"
     ]
    }
   ],
   "source": [
    "all_Test_words = [word for tokens in test_df[\"tokens\"] for word in tokens]\n",
    "Test_sentence_lengths = [len(tokens) for tokens in test_df[\"tokens\"]]\n",
    "r_TEST_VOCAB = sorted(list(set(all_Test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_Test_words), len(r_TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(Test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_sequences = tokenizer.texts_to_sequences(test_df[\"Tweet\"].tolist())\n",
    "Test_nn = pad_sequences(Test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_y_pred = clf.predict(Test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df['Tweet_submission_id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_id,r_y_pred, \"./Submissions/NN_W2V_SUB.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
